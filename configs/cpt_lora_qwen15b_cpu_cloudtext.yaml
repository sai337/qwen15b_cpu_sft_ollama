run:
  task: cpt   # continued pretraining on raw text
  mode: lora
  run_name: cpt_lora_qwen15b_cpu_cloudtext

compute:
  device: cpu
  dtype: float32
  num_threads: 16          # m5.8xlarge: 16 physical cores
  interop_threads: 2

model:
  base_model: Qwen/Qwen2.5-1.5B-Instruct
  init: pretrained
  max_length: 256

# Raw text datasets. These are not instruction/response pairs.
# This stage helps the model pick up domain terminology before SFT.
data:
  streaming: true
  shuffle_buffer: 2000
  text_sources:
    - name: aws_whitepapers
      dataset_name: si3mshady/aws_whitepapers
      dataset_config: null
      split: train
      text_field: text
      weight: 1.0

    - name: deepfabric_devops_with_tools
      dataset_name: alwaysfurther/deepfabric-devops-with-tools
      dataset_config: null
      split: train
      text_field: text
      weight: 0.7

    - name: devops_llm
      dataset_name: ahmedgongi/Devops_LLM
      dataset_config: null
      split: train
      text_field: text
      weight: 0.7

    - name: devops_mubeen
      dataset_name: Mubeen161/DEVOPS
      dataset_config: null
      split: train
      text_field: text
      weight: 0.5

train:
  output_dir: outputs
  max_steps: 300
  per_device_batch_size: 1
  grad_accum_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.0
  warmup_steps: 20
  max_grad_norm: 1.0
  log_every: 5
  save_every: 50
  seed: 123

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  # Qwen2.x uses standard attention projection names.
  target_modules: ["q_proj", "v_proj"]
